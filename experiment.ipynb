{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-17T16:47:45.139746900Z",
     "start_time": "2023-05-17T16:47:45.076799600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T16:47:45.139746900Z",
     "start_time": "2023-05-17T16:47:45.092515300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data from pickle file\n",
    "data = pd.read_pickle('ass2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T16:47:46.154019900Z",
     "start_time": "2023-05-17T16:47:45.139746900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (40533, 43)\n",
      "Dev set: (13512, 43)\n",
      "Test set: (13512, 43)\n",
      "number of labels: 3\n",
      "number of samples in train set: 40533\n",
      "number of samples in dev set: 13512\n",
      "number of samples in test set: 13512\n",
      " the first 5 samples in train set:        f0  f1  f2  f3  f4  f5  f6  f7  f8  f9  ...  f33  f34  f35  f36  f37   \n",
      "51905   1   0   0   0   0   0   2   1   2   2  ...    0    0    0    2    0  \\\n",
      "52612   0   0   0   0   0   0   2   1   0   0  ...    0    0    0    2    0   \n",
      "61699   2   1   2   1   1   0   2   2   0   0  ...    0    0    0    1    0   \n",
      "6291    0   0   0   0   0   0   0   0   0   0  ...    0    0    0    2    0   \n",
      "17484   0   0   0   0   0   0   1   1   2   0  ...    0    0    0    2    1   \n",
      "\n",
      "       f38  f39  f40  f41  target  \n",
      "51905    0    0    0    0       2  \n",
      "52612    0    0    0    0       2  \n",
      "61699    0    0    0    0       2  \n",
      "6291     0    0    0    0       2  \n",
      "17484    2    0    0    0       2  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "number of samples in class in train set 0: 3917\n",
      "number of samples in class in train set 1: 9882\n",
      "number of samples in class in train set 2: 26734\n",
      "range of feature 0: (2, 0)\n",
      "range of feature 1: (2, 0)\n",
      "range of feature 2: (2, 0)\n",
      "range of feature 3: (2, 0)\n",
      "range of feature 4: (2, 0)\n",
      "range of feature 5: (2, 0)\n",
      "range of feature 6: (2, 0)\n",
      "range of feature 7: (2, 0)\n",
      "range of feature 8: (2, 0)\n",
      "range of feature 9: (2, 0)\n",
      "range of feature 10: (2, 0)\n",
      "range of feature 11: (2, 0)\n",
      "range of feature 12: (2, 0)\n",
      "range of feature 13: (2, 0)\n",
      "range of feature 14: (2, 0)\n",
      "range of feature 15: (2, 0)\n",
      "range of feature 16: (2, 0)\n",
      "range of feature 17: (2, 0)\n",
      "range of feature 18: (2, 0)\n",
      "range of feature 19: (2, 0)\n",
      "range of feature 20: (2, 0)\n",
      "range of feature 21: (2, 0)\n",
      "range of feature 22: (2, 0)\n",
      "range of feature 23: (2, 0)\n",
      "range of feature 24: (2, 0)\n",
      "range of feature 25: (2, 0)\n",
      "range of feature 26: (2, 0)\n",
      "range of feature 27: (2, 0)\n",
      "range of feature 28: (2, 0)\n",
      "range of feature 29: (2, 0)\n",
      "range of feature 30: (2, 0)\n",
      "range of feature 31: (2, 0)\n",
      "range of feature 32: (2, 0)\n",
      "range of feature 33: (2, 0)\n",
      "range of feature 34: (2, 0)\n",
      "range of feature 35: (2, 0)\n",
      "range of feature 36: (2, 0)\n",
      "range of feature 37: (2, 0)\n",
      "range of feature 38: (2, 0)\n",
      "range of feature 39: (2, 0)\n",
      "range of feature 40: (2, 0)\n",
      "range of feature 41: (2, 0)\n",
      "number of Nan values in feature 0: 0\n",
      "number of Nan values in feature 1: 0\n",
      "number of Nan values in feature 2: 0\n",
      "number of Nan values in feature 3: 0\n",
      "number of Nan values in feature 4: 0\n",
      "number of Nan values in feature 5: 0\n",
      "number of Nan values in feature 6: 0\n",
      "number of Nan values in feature 7: 0\n",
      "number of Nan values in feature 8: 0\n",
      "number of Nan values in feature 9: 0\n",
      "number of Nan values in feature 10: 0\n",
      "number of Nan values in feature 11: 0\n",
      "number of Nan values in feature 12: 0\n",
      "number of Nan values in feature 13: 0\n",
      "number of Nan values in feature 14: 0\n",
      "number of Nan values in feature 15: 0\n",
      "number of Nan values in feature 16: 0\n",
      "number of Nan values in feature 17: 0\n",
      "number of Nan values in feature 18: 0\n",
      "number of Nan values in feature 19: 0\n",
      "number of Nan values in feature 20: 0\n",
      "number of Nan values in feature 21: 0\n",
      "number of Nan values in feature 22: 0\n",
      "number of Nan values in feature 23: 0\n",
      "number of Nan values in feature 24: 0\n",
      "number of Nan values in feature 25: 0\n",
      "number of Nan values in feature 26: 0\n",
      "number of Nan values in feature 27: 0\n",
      "number of Nan values in feature 28: 0\n",
      "number of Nan values in feature 29: 0\n",
      "number of Nan values in feature 30: 0\n",
      "number of Nan values in feature 31: 0\n",
      "number of Nan values in feature 32: 0\n",
      "number of Nan values in feature 33: 0\n",
      "number of Nan values in feature 34: 0\n",
      "number of Nan values in feature 35: 0\n",
      "number of Nan values in feature 36: 0\n",
      "number of Nan values in feature 37: 0\n",
      "number of Nan values in feature 38: 0\n",
      "number of Nan values in feature 39: 0\n",
      "number of Nan values in feature 40: 0\n",
      "number of Nan values in feature 41: 0\n",
      "type of feature 0: int64\n",
      "type of feature 1: int64\n",
      "type of feature 2: int64\n",
      "type of feature 3: int64\n",
      "type of feature 4: int64\n",
      "type of feature 5: int64\n",
      "type of feature 6: int64\n",
      "type of feature 7: int64\n",
      "type of feature 8: int64\n",
      "type of feature 9: int64\n",
      "type of feature 10: int64\n",
      "type of feature 11: int64\n",
      "type of feature 12: int64\n",
      "type of feature 13: int64\n",
      "type of feature 14: int64\n",
      "type of feature 15: int64\n",
      "type of feature 16: int64\n",
      "type of feature 17: int64\n",
      "type of feature 18: int64\n",
      "type of feature 19: int64\n",
      "type of feature 20: int64\n",
      "type of feature 21: int64\n",
      "type of feature 22: int64\n",
      "type of feature 23: int64\n",
      "type of feature 24: int64\n",
      "type of feature 25: int64\n",
      "type of feature 26: int64\n",
      "type of feature 27: int64\n",
      "type of feature 28: int64\n",
      "type of feature 29: int64\n",
      "type of feature 30: int64\n",
      "type of feature 31: int64\n",
      "type of feature 32: int64\n",
      "type of feature 33: int64\n",
      "type of feature 34: int64\n",
      "type of feature 35: int64\n",
      "type of feature 36: int64\n",
      "type of feature 37: int64\n",
      "type of feature 38: int64\n",
      "type of feature 39: int64\n",
      "type of feature 40: int64\n",
      "type of feature 41: int64\n",
      "number of unique values in feature 0: 3\n",
      "number of unique values in feature 1: 3\n",
      "number of unique values in feature 2: 3\n",
      "number of unique values in feature 3: 3\n",
      "number of unique values in feature 4: 3\n",
      "number of unique values in feature 5: 3\n",
      "number of unique values in feature 6: 3\n",
      "number of unique values in feature 7: 3\n",
      "number of unique values in feature 8: 3\n",
      "number of unique values in feature 9: 3\n",
      "number of unique values in feature 10: 3\n",
      "number of unique values in feature 11: 3\n",
      "number of unique values in feature 12: 3\n",
      "number of unique values in feature 13: 3\n",
      "number of unique values in feature 14: 3\n",
      "number of unique values in feature 15: 3\n",
      "number of unique values in feature 16: 3\n",
      "number of unique values in feature 17: 3\n",
      "number of unique values in feature 18: 3\n",
      "number of unique values in feature 19: 3\n",
      "number of unique values in feature 20: 3\n",
      "number of unique values in feature 21: 3\n",
      "number of unique values in feature 22: 3\n",
      "number of unique values in feature 23: 3\n",
      "number of unique values in feature 24: 3\n",
      "number of unique values in feature 25: 3\n",
      "number of unique values in feature 26: 3\n",
      "number of unique values in feature 27: 3\n",
      "number of unique values in feature 28: 3\n",
      "number of unique values in feature 29: 3\n",
      "number of unique values in feature 30: 3\n",
      "number of unique values in feature 31: 3\n",
      "number of unique values in feature 32: 3\n",
      "number of unique values in feature 33: 3\n",
      "number of unique values in feature 34: 3\n",
      "number of unique values in feature 35: 3\n",
      "number of unique values in feature 36: 3\n",
      "number of unique values in feature 37: 3\n",
      "number of unique values in feature 38: 3\n",
      "number of unique values in feature 39: 3\n",
      "number of unique values in feature 40: 3\n",
      "number of unique values in feature 41: 3\n",
      "number of samples in feature 0: value 0: 14953 , value 1: 12267, value 2:13313\n",
      "number of samples in feature 1: value 0: 25922 , value 1: 7332, value 2:7279\n",
      "number of samples in feature 2: value 0: 33157 , value 1: 3772, value 2:3604\n",
      "number of samples in feature 3: value 0: 36947 , value 1: 1832, value 2:1754\n",
      "number of samples in feature 4: value 0: 39132 , value 1: 695, value 2:706\n",
      "number of samples in feature 5: value 0: 40224 , value 1: 219, value 2:90\n",
      "number of samples in feature 6: value 0: 13018 , value 1: 11892, value 2:15623\n",
      "number of samples in feature 7: value 0: 24751 , value 1: 7934, value 2:7848\n",
      "number of samples in feature 8: value 0: 32664 , value 1: 4048, value 2:3821\n",
      "number of samples in feature 9: value 0: 36779 , value 1: 1918, value 2:1836\n",
      "number of samples in feature 10: value 0: 39175 , value 1: 725, value 2:633\n",
      "number of samples in feature 11: value 0: 40241 , value 1: 204, value 2:88\n",
      "number of samples in feature 12: value 0: 9727 , value 1: 11582, value 2:19224\n",
      "number of samples in feature 13: value 0: 22603 , value 1: 9229, value 2:8701\n",
      "number of samples in feature 14: value 0: 31805 , value 1: 4563, value 2:4165\n",
      "number of samples in feature 15: value 0: 36365 , value 1: 2203, value 2:1965\n",
      "number of samples in feature 16: value 0: 39062 , value 1: 791, value 2:680\n",
      "number of samples in feature 17: value 0: 40225 , value 1: 217, value 2:91\n",
      "number of samples in feature 18: value 0: 17454 , value 1: 11337, value 2:11742\n",
      "number of samples in feature 19: value 0: 28199 , value 1: 6103, value 2:6231\n",
      "number of samples in feature 20: value 0: 34727 , value 1: 2939, value 2:2867\n",
      "number of samples in feature 21: value 0: 37901 , value 1: 1390, value 2:1242\n",
      "number of samples in feature 22: value 0: 39634 , value 1: 479, value 2:420\n",
      "number of samples in feature 23: value 0: 40340 , value 1: 136, value 2:57\n",
      "number of samples in feature 24: value 0: 24517 , value 1: 11385, value 2:4631\n",
      "number of samples in feature 25: value 0: 33308 , value 1: 3364, value 2:3861\n",
      "number of samples in feature 26: value 0: 37411 , value 1: 1454, value 2:1668\n",
      "number of samples in feature 27: value 0: 39301 , value 1: 673, value 2:559\n",
      "number of samples in feature 28: value 0: 40169 , value 1: 208, value 2:156\n",
      "number of samples in feature 29: value 0: 40489 , value 1: 32, value 2:12\n",
      "number of samples in feature 30: value 0: 20505 , value 1: 11399, value 2:8629\n",
      "number of samples in feature 31: value 0: 30590 , value 1: 4989, value 2:4954\n",
      "number of samples in feature 32: value 0: 36138 , value 1: 2181, value 2:2214\n",
      "number of samples in feature 33: value 0: 38869 , value 1: 926, value 2:738\n",
      "number of samples in feature 34: value 0: 40105 , value 1: 253, value 2:175\n",
      "number of samples in feature 35: value 0: 40480 , value 1: 41, value 2:12\n",
      "number of samples in feature 36: value 0: 17827 , value 1: 11415, value 2:11291\n",
      "number of samples in feature 37: value 0: 28900 , value 1: 5945, value 2:5688\n",
      "number of samples in feature 38: value 0: 35383 , value 1: 2656, value 2:2494\n",
      "number of samples in feature 39: value 0: 38584 , value 1: 1061, value 2:888\n",
      "number of samples in feature 40: value 0: 40056 , value 1: 304, value 2:173\n",
      "number of samples in feature 41: value 0: 40485 , value 1: 39, value 2:9\n",
      "number of samples in all the train set: label 0: 1378122 , label 1: 162132, label 2:162132\n",
      "dominant features: [3, 4, 5, 9, 10, 11, 16, 17, 21, 22, 23, 26, 27, 28, 29, 33, 34, 35, 39, 40, 41]\n",
      "number of duplicates in train set: 0\n",
      "mean of feature 0: 0.9595391409468828, std of feature 0: 0.8340986916537341\n",
      "mean of feature 1: 0.540053783337034, std of feature 1: 0.7794612449014806\n",
      "mean of feature 2: 0.2708903856117238, std of feature 2: 0.612649324152414\n",
      "mean of feature 3: 0.13174450447783287, std of feature 3: 0.44825735284829593\n",
      "mean of feature 4: 0.05198233538104754, std of feature 4: 0.2900275590075759\n",
      "mean of feature 5: 0.009843830952557175, std of feature 5: 0.11911236685739006\n",
      "mean of feature 6: 1.064268620630104, std of feature 6: 0.8381401874126808\n",
      "mean of feature 7: 0.5829817679421706, std of feature 7: 0.7939483943142325\n",
      "mean of feature 8: 0.2884069770310611, std of feature 8: 0.6275078703221054\n",
      "mean of feature 9: 0.13791231835788123, std of feature 9: 0.4576957079470878\n",
      "mean of feature 10: 0.0491204697407051, std of feature 10: 0.27917997543112305\n",
      "mean of feature 11: 0.0093750770976735, std of feature 11: 0.11674470437086311\n",
      "mean of feature 12: 1.234302913675277, std of feature 12: 0.8120096447754765\n",
      "mean of feature 13: 0.657020205758271, std of feature 13: 0.8091191775913088\n",
      "mean of feature 14: 0.3180864974218538, std of feature 14: 0.6499377170644601\n",
      "mean of feature 15: 0.15130881010534625, std of feature 15: 0.47473412378320856\n",
      "mean of feature 16: 0.05306787062393605, std of feature 16: 0.28949020577359835\n",
      "mean of feature 17: 0.009843830952557175, std of feature 17: 0.11931931297744662\n",
      "mean of feature 18: 0.8590777884686551, std of feature 18: 0.8369246719177791\n",
      "mean of feature 19: 0.4580218587323909, std of feature 19: 0.7454468605798584\n",
      "mean of feature 20: 0.21397379912663755, std of feature 20: 0.5564656247844425\n",
      "mean of feature 21: 0.09557644388522932, std of feature 21: 0.38435008257010095\n",
      "mean of feature 22: 0.03254138603113513, std of feature 22: 0.22848697744788485\n",
      "mean of feature 23: 0.006167813880048356, std of feature 23: 0.09456370911344102\n",
      "mean of feature 24: 0.5093874127254336, std of feature 24: 0.6916769803241639\n",
      "mean of feature 25: 0.2735055386968643, std of feature 25: 0.6238683307605347\n",
      "mean of feature 26: 0.11817531394172649, std of feature 26: 0.43187176051678366\n",
      "mean of feature 27: 0.04418621863666642, std of feature 27: 0.26422766772394485\n",
      "mean of feature 28: 0.01282905287050058, std of feature 28: 0.1426951295428363\n",
      "mean of feature 29: 0.0013815903091308316, std of feature 29: 0.0444048606554867\n",
      "mean of feature 30: 0.7070041694421829, std of feature 30: 0.7955663401634399\n",
      "mean of feature 31: 0.3675276934843214, std of feature 31: 0.690575041324614\n",
      "mean of feature 32: 0.16305232773295833, std of feature 32: 0.4956920269655734\n",
      "mean of feature 33: 0.0592603557595046, std of feature 33: 0.30358415495965374\n",
      "mean of feature 34: 0.014876767078676634, std of feature 34: 0.15261188784636384\n",
      "mean of feature 35: 0.0016036316088125725, std of feature 35: 0.046831294098715996\n",
      "mean of feature 36: 0.8387486739200158, std of feature 36: 0.8320911176514497\n",
      "mean of feature 37: 0.42733081686527025, std of feature 37: 0.724830595808601\n",
      "mean of feature 38: 0.18858707719635853, std of feature 38: 0.5254352615171711\n",
      "mean of feature 39: 0.06999235191078874, std of feature 39: 0.33001450330045884\n",
      "mean of feature 40: 0.016036316088125724, std of feature 40: 0.15593398303267433\n",
      "mean of feature 41: 0.0014062615646510252, std of feature 41: 0.042992634164776326\n"
     ]
    }
   ],
   "source": [
    "# dividing into 'train', 'dev', 'test' sets\n",
    "train = data['train']\n",
    "dev = data['dev']\n",
    "test = data['test']\n",
    "X_train = train.drop('target', axis=1)\n",
    "y_train = train['target']\n",
    "X_dev = dev.drop('target', axis=1)\n",
    "y_dev = dev['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "y_test = test['target']\n",
    "print(f\"Train set: {train.shape}\")\n",
    "print(f\"Dev set: {dev.shape}\")\n",
    "print(f\"Test set: {test.shape}\")\n",
    "print(f\"number of labels: {len(np.unique(train['target']))}\")\n",
    "print(f\"number of samples in train set: {len(train['f0'])}\")\n",
    "print(f\"number of samples in dev set: {len(dev['f0'])}\")\n",
    "print(f\"number of samples in test set: {len(test['f0'])}\")\n",
    "print(f\" the first 5 samples in train set: {train.head()}\")\n",
    "for i in range(len(np.unique(train['target']))):\n",
    "    print(f\"number of samples in class in train set {i}: {len(train[train['target']==i])}\")\n",
    "# range per feature\n",
    "feature_values = [train[f'f{i}'] for i in range(0, 42)]\n",
    "for i in range(len(feature_values)):\n",
    "    print(f\"range of feature {i}: {np.max(feature_values[i]), np.min(feature_values[i])}\")\n",
    "# check if there are any Null values\n",
    "for i in range(42):\n",
    "    # train.isnull().sum()\n",
    "    print(f\"number of Nan values in feature {i}: {np.sum(np.isnan(train[f'f{i}']))}\")\n",
    "# check what type each feature is\n",
    "for i in range(42):\n",
    "    print(f\"type of feature {i}: {train[f'f{i}'][0].dtype}\")\n",
    "# how many unique values each feature has\n",
    "for i in range(42):\n",
    "    print(f\"number of unique values in feature {i}: {len(np.unique(train[f'f{i}']))}\")\n",
    "# check how many samples are there for each label in the train set\n",
    "for i in range(42):\n",
    "    print(f\"number of samples in feature {i}: value 0: {len(train[train[f'f{i}']==0])} , value 1: {len(train[train[f'f{i}']==1])}, value 2:{len(train[train[f'f{i}']==2])}\")\n",
    "# check how many samples are there for each label in the all the train set from all the features together\n",
    "labels = [0,0,0]\n",
    "for i in range(42):\n",
    "    labels[0] += len(train[train[f'f{i}']==0])\n",
    "    labels[1] += len(train[train[f'f{i}']==1])\n",
    "    labels[2] += len(train[train[f'f{i}']==2])\n",
    "print(f\"number of samples in all the train set: label 0: {labels[0]} , label 1: {labels[1]}, label 2:{labels[2]}\")\n",
    "# check if is there dominant label in each feature (more than 90% of the samples) and save those features\n",
    "dominant_features = []\n",
    "for i in range(42):\n",
    "    if len(train[train[f'f{i}']==0])/len(train[f'f{i}']) > 0.90:\n",
    "        dominant_features.append(i)\n",
    "print(f\"dominant features: {dominant_features}\")\n",
    "# check if there are duplicates samples in the train set\n",
    "print(f\"number of duplicates in train set: {len(train[train.duplicated()])}\")\n",
    "# check the mean and std of each feature\n",
    "for i in range(42):\n",
    "    print(f\"mean of feature {i}: {np.mean(train[f'f{i}'])}, std of feature {i}: {np.std(train[f'f{i}'])}\")\n",
    "\n",
    "# sns.pairplot(train ,hue=\"target\", palette='Set1');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation of data analysis and preprocessing:\n",
    "We checked that each of the sets (train, dev, test) has the same number of features and labels.\n",
    "We checked the range of each feature and saw that all the features are in the same range.\n",
    "We checked that all the features are the same type (int64), so we didn't need to change any type or convert any feature.\n",
    "We checked that there are no null values in the data, so we didn't need to fill any values.\n",
    "We checked the values for each feature and saw that there are only 3 values for each feature (0,1,2).\n",
    "Also, we counted the number of samples for each feature for each value, and saw that there are no features that have only one value, and since we don't know that is the meaning of each value, we didn't remove any feature (even though there are some features that dominant in one value).\n",
    "We calculated the mean and the standard deviation for each feature, we did it for checking the statistical behavior of each feature.\n",
    "We checked that there are no duplicates in the data, so we didn't need to remove any duplicates.\n",
    "We checked that there are no dominant features in the data, so we didn't need to remove any features.\n",
    "We checked that there are no dominant labels in the data, so we didn't need to remove any labels.\n",
    "We checked that there are no highly correlated features in the data, so we didn't need to remove any features.\n",
    "We checked that there are no highly correlated features with the target in the data, so we didn't need to remove any features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T16:47:47.080895100Z",
     "start_time": "2023-05-17T16:47:46.154019900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target    1.000000\n",
      "f18       0.074426\n",
      "f7        0.070938\n",
      "f31       0.054176\n",
      "f25       0.045310\n",
      "f13       0.042819\n",
      "f19       0.042409\n",
      "f26       0.038319\n",
      "f20       0.028279\n",
      "f33       0.027202\n",
      "f32       0.026104\n",
      "f24       0.024502\n",
      "f9        0.021572\n",
      "f30       0.015781\n",
      "f14       0.012039\n",
      "f8        0.006194\n",
      "f27       0.002367\n",
      "f29       0.000399\n",
      "f22      -0.000081\n",
      "f23      -0.000288\n",
      "f10      -0.001704\n",
      "f28      -0.002536\n",
      "f34      -0.003035\n",
      "f21      -0.003561\n",
      "f11      -0.005711\n",
      "f35      -0.008417\n",
      "f17      -0.009548\n",
      "f5       -0.010190\n",
      "f41      -0.010464\n",
      "f15      -0.012029\n",
      "f40      -0.020028\n",
      "f16      -0.021968\n",
      "f12      -0.022053\n",
      "f39      -0.022334\n",
      "f38      -0.028690\n",
      "f3       -0.035043\n",
      "f4       -0.035689\n",
      "f6       -0.037590\n",
      "f2       -0.039243\n",
      "f37      -0.047933\n",
      "f36      -0.063196\n",
      "f1       -0.063723\n",
      "f0       -0.089957\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check correlation between features\n",
    "corr_matrix = train.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# check correlation between features and target\n",
    "corr_matrix = train.corr()\n",
    "print(corr_matrix['target'].sort_values(ascending=False))\n",
    "\n",
    "# scaling features\n",
    "scalar = StandardScaler()\n",
    "\n",
    "# fit and transform scalar to train set\n",
    "X_train_scaled = scalar.fit_transform(X_train)\n",
    "\n",
    "# transform test set\n",
    "X_test_scaled = scalar.transform(X_test)\n",
    "\n",
    "# transform test set\n",
    "X_dev_scaled = scalar.transform(X_dev)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# highly correlated features with each other => remove one of them\n",
    "# highly correlated features with target => keep them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T16:47:47.147443500Z",
     "start_time": "2023-05-17T16:47:47.091795500Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the models\n",
    "# models = [('LogisticRegression', LogisticRegression()), ('Naive Bayes', GaussianNB()), ('KNN', KNeighborsClassifier()), ('SVM', SVC()), ('RF', RandomForestClassifier()), ('Decision Tree', DecisionTreeClassifier()), ('AdaBoost', AdaBoostClassifier()), ('GradientBoosting', GradientBoostingClassifier()), ('XGBoost', XGBClassifier())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T17:27:11.843531800Z",
     "start_time": "2023-05-17T16:47:47.099839900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: penalty = C = 0.1, max_iter = 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.57      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.58      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 0.1, max_iter = 1000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.57      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.58      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 0.1, max_iter = 2500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.57      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.58      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 1, max_iter = 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.56      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 1, max_iter = 1000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.56      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 1, max_iter = 2500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.56      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 10, max_iter = 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.56      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 10, max_iter = 1000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.56      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "LogisticRegression: penalty = C = 10, max_iter = 2500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.56      0.03      0.06      3373\n",
      "   LABEL = 2       0.66      0.99      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.14      0.11      0.13      1270\n",
      "   LABEL = 1       0.39      0.19      0.25      3373\n",
      "   LABEL = 2       0.69      0.84      0.76      8869\n",
      "\n",
      "    accuracy                           0.61     13512\n",
      "   macro avg       0.41      0.38      0.38     13512\n",
      "weighted avg       0.56      0.61      0.57     13512\n",
      "\n",
      "KNN: n_neighbors = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.24      0.29      0.26      1270\n",
      "   LABEL = 1       0.61      0.45      0.52      3373\n",
      "   LABEL = 2       0.80      0.85      0.82      8869\n",
      "\n",
      "    accuracy                           0.70     13512\n",
      "   macro avg       0.55      0.53      0.54     13512\n",
      "weighted avg       0.70      0.70      0.69     13512\n",
      "\n",
      "KNN: n_neighbors = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.31      0.23      0.26      1270\n",
      "   LABEL = 1       0.62      0.51      0.56      3373\n",
      "   LABEL = 2       0.80      0.88      0.84      8869\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.57      0.54      0.55     13512\n",
      "weighted avg       0.71      0.73      0.71     13512\n",
      "\n",
      "KNN: n_neighbors = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.35      0.16      0.21      1270\n",
      "   LABEL = 1       0.66      0.51      0.57      3373\n",
      "   LABEL = 2       0.78      0.92      0.85      8869\n",
      "\n",
      "    accuracy                           0.74     13512\n",
      "   macro avg       0.60      0.53      0.54     13512\n",
      "weighted avg       0.71      0.74      0.72     13512\n",
      "\n",
      "RandomForest: n_estimators = 100, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.00      0.00      0.00      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.22      0.33      0.26     13512\n",
      "weighted avg       0.43      0.66      0.52     13512\n",
      "\n",
      "RandomForest: n_estimators = 100, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.88      0.01      0.01      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.51      0.34      0.27     13512\n",
      "weighted avg       0.65      0.66      0.52     13512\n",
      "\n",
      "RandomForest: n_estimators = 100, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.88      0.07      0.14      3373\n",
      "   LABEL = 2       0.67      1.00      0.80      8869\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.52      0.36      0.31     13512\n",
      "weighted avg       0.66      0.67      0.56     13512\n",
      "\n",
      "RandomForest: n_estimators = 200, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.00      0.00      0.00      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.22      0.33      0.26     13512\n",
      "weighted avg       0.43      0.66      0.52     13512\n",
      "\n",
      "RandomForest: n_estimators = 200, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.88      0.00      0.01      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.51      0.33      0.27     13512\n",
      "weighted avg       0.65      0.66      0.52     13512\n",
      "\n",
      "RandomForest: n_estimators = 200, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.93      0.07      0.12      3373\n",
      "   LABEL = 2       0.67      1.00      0.80      8869\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.53      0.36      0.31     13512\n",
      "weighted avg       0.67      0.67      0.56     13512\n",
      "\n",
      "RandomForest: n_estimators = 500, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.00      0.00      0.00      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.22      0.33      0.26     13512\n",
      "weighted avg       0.43      0.66      0.52     13512\n",
      "\n",
      "RandomForest: n_estimators = 500, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       1.00      0.00      0.01      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.55      0.33      0.27     13512\n",
      "weighted avg       0.68      0.66      0.52     13512\n",
      "\n",
      "RandomForest: n_estimators = 500, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.90      0.07      0.13      3373\n",
      "   LABEL = 2       0.67      1.00      0.80      8869\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.52      0.36      0.31     13512\n",
      "weighted avg       0.66      0.67      0.56     13512\n",
      "\n",
      "DecisionTree: max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.48      0.13      0.21      3373\n",
      "   LABEL = 2       0.68      0.96      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.39      0.36      0.33     13512\n",
      "weighted avg       0.56      0.66      0.57     13512\n",
      "\n",
      "DecisionTree: max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.53      0.21      0.30      3373\n",
      "   LABEL = 2       0.69      0.95      0.80      8869\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.41      0.39      0.37     13512\n",
      "weighted avg       0.59      0.67      0.60     13512\n",
      "\n",
      "DecisionTree: max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.56      0.37      0.45      3373\n",
      "   LABEL = 2       0.72      0.92      0.81      8869\n",
      "\n",
      "    accuracy                           0.70     13512\n",
      "   macro avg       0.43      0.43      0.42     13512\n",
      "weighted avg       0.61      0.70      0.64     13512\n",
      "\n",
      "AdaBoost: n_estimators = 100, learning_rate = 0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.91      0.02      0.04      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.52      0.34      0.28     13512\n",
      "weighted avg       0.66      0.66      0.53     13512\n",
      "\n",
      "AdaBoost: n_estimators = 100, learning_rate = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.70      0.50      0.58      3373\n",
      "   LABEL = 2       0.75      0.94      0.84      8869\n",
      "\n",
      "    accuracy                           0.74     13512\n",
      "   macro avg       0.48      0.48      0.47     13512\n",
      "weighted avg       0.67      0.74      0.70     13512\n",
      "\n",
      "AdaBoost: n_estimators = 100, learning_rate = 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.00      0.00      0.00      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.22      0.33      0.26     13512\n",
      "weighted avg       0.43      0.66      0.52     13512\n",
      "\n",
      "AdaBoost: n_estimators = 200, learning_rate = 0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.81      0.10      0.17      3373\n",
      "   LABEL = 2       0.67      0.99      0.80      8869\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.49      0.36      0.32     13512\n",
      "weighted avg       0.64      0.68      0.57     13512\n",
      "\n",
      "AdaBoost: n_estimators = 200, learning_rate = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.69      0.55      0.61      3373\n",
      "   LABEL = 2       0.77      0.93      0.84      8869\n",
      "\n",
      "    accuracy                           0.75     13512\n",
      "   macro avg       0.49      0.49      0.48     13512\n",
      "weighted avg       0.67      0.75      0.71     13512\n",
      "\n",
      "AdaBoost: n_estimators = 200, learning_rate = 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.00      0.00      0.00      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.22      0.33      0.26     13512\n",
      "weighted avg       0.43      0.66      0.52     13512\n",
      "\n",
      "AdaBoost: n_estimators = 500, learning_rate = 0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.74      0.31      0.43      3373\n",
      "   LABEL = 2       0.71      0.97      0.82      8869\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.48      0.43      0.42     13512\n",
      "weighted avg       0.65      0.71      0.65     13512\n",
      "\n",
      "AdaBoost: n_estimators = 500, learning_rate = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.43      0.00      0.00      1270\n",
      "   LABEL = 1       0.68      0.60      0.64      3373\n",
      "   LABEL = 2       0.78      0.93      0.85      8869\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.63      0.51      0.50     13512\n",
      "weighted avg       0.72      0.76      0.72     13512\n",
      "\n",
      "AdaBoost: n_estimators = 500, learning_rate = 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.00      0.00      0.00      3373\n",
      "   LABEL = 2       0.66      1.00      0.79      8869\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.22      0.33      0.26     13512\n",
      "weighted avg       0.43      0.66      0.52     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.05, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.75      0.37      0.50      3373\n",
      "   LABEL = 2       0.73      0.97      0.83      8869\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.49      0.45      0.44     13512\n",
      "weighted avg       0.66      0.73      0.67     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.05, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.53      0.02      0.04      1270\n",
      "   LABEL = 1       0.75      0.56      0.64      3373\n",
      "   LABEL = 2       0.77      0.96      0.86      8869\n",
      "\n",
      "    accuracy                           0.77     13512\n",
      "   macro avg       0.69      0.51      0.51     13512\n",
      "weighted avg       0.75      0.77      0.73     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.05, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.54      0.07      0.12      1270\n",
      "   LABEL = 1       0.78      0.67      0.72      3373\n",
      "   LABEL = 2       0.82      0.96      0.88      8869\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.71      0.57      0.57     13512\n",
      "weighted avg       0.78      0.80      0.77     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.73      0.01      0.02      1270\n",
      "   LABEL = 1       0.73      0.50      0.60      3373\n",
      "   LABEL = 2       0.76      0.96      0.85      8869\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.74      0.49      0.49     13512\n",
      "weighted avg       0.75      0.76      0.71     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.58      0.05      0.09      1270\n",
      "   LABEL = 1       0.76      0.67      0.71      3373\n",
      "   LABEL = 2       0.81      0.96      0.88      8869\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.72      0.56      0.56     13512\n",
      "weighted avg       0.78      0.80      0.76     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.54      0.13      0.22      1270\n",
      "   LABEL = 1       0.78      0.74      0.76      3373\n",
      "   LABEL = 2       0.85      0.96      0.90      8869\n",
      "\n",
      "    accuracy                           0.83     13512\n",
      "   macro avg       0.72      0.61      0.63     13512\n",
      "weighted avg       0.80      0.83      0.80     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.5, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.55      0.08      0.14      1270\n",
      "   LABEL = 1       0.73      0.68      0.71      3373\n",
      "   LABEL = 2       0.82      0.94      0.88      8869\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.70      0.57      0.57     13512\n",
      "weighted avg       0.77      0.80      0.76     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.5, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.49      0.22      0.30      1270\n",
      "   LABEL = 1       0.78      0.78      0.78      3373\n",
      "   LABEL = 2       0.87      0.94      0.91      8869\n",
      "\n",
      "    accuracy                           0.83     13512\n",
      "   macro avg       0.71      0.65      0.66     13512\n",
      "weighted avg       0.81      0.83      0.82     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 100, learning_rate = 0.5, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.48      0.32      0.38      1270\n",
      "   LABEL = 1       0.81      0.80      0.80      3373\n",
      "   LABEL = 2       0.89      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.69      0.70     13512\n",
      "weighted avg       0.83      0.85      0.84     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.05, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.82      0.01      0.01      1270\n",
      "   LABEL = 1       0.74      0.50      0.60      3373\n",
      "   LABEL = 2       0.76      0.96      0.85      8869\n",
      "\n",
      "    accuracy                           0.75     13512\n",
      "   macro avg       0.77      0.49      0.49     13512\n",
      "weighted avg       0.76      0.75      0.71     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.05, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.56      0.05      0.09      1270\n",
      "   LABEL = 1       0.76      0.66      0.71      3373\n",
      "   LABEL = 2       0.81      0.95      0.88      8869\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.71      0.55      0.56     13512\n",
      "weighted avg       0.77      0.80      0.76     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.05, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.56      0.13      0.22      1270\n",
      "   LABEL = 1       0.79      0.75      0.77      3373\n",
      "   LABEL = 2       0.85      0.96      0.90      8869\n",
      "\n",
      "    accuracy                           0.83     13512\n",
      "   macro avg       0.73      0.61      0.63     13512\n",
      "weighted avg       0.81      0.83      0.80     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.59      0.03      0.05      1270\n",
      "   LABEL = 1       0.73      0.60      0.66      3373\n",
      "   LABEL = 2       0.79      0.95      0.86      8869\n",
      "\n",
      "    accuracy                           0.78     13512\n",
      "   macro avg       0.70      0.53      0.52     13512\n",
      "weighted avg       0.76      0.78      0.73     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.54      0.11      0.18      1270\n",
      "   LABEL = 1       0.77      0.73      0.75      3373\n",
      "   LABEL = 2       0.84      0.95      0.89      8869\n",
      "\n",
      "    accuracy                           0.82     13512\n",
      "   macro avg       0.72      0.60      0.61     13512\n",
      "weighted avg       0.79      0.82      0.79     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.51      0.21      0.30      1270\n",
      "   LABEL = 1       0.79      0.78      0.79      3373\n",
      "   LABEL = 2       0.87      0.95      0.91      8869\n",
      "\n",
      "    accuracy                           0.84     13512\n",
      "   macro avg       0.73      0.65      0.66     13512\n",
      "weighted avg       0.82      0.84      0.82     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.5, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.49      0.13      0.20      1270\n",
      "   LABEL = 1       0.74      0.72      0.73      3373\n",
      "   LABEL = 2       0.84      0.94      0.88      8869\n",
      "\n",
      "    accuracy                           0.81     13512\n",
      "   macro avg       0.69      0.59      0.60     13512\n",
      "weighted avg       0.78      0.81      0.78     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.5, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.48      0.28      0.36      1270\n",
      "   LABEL = 1       0.80      0.80      0.80      3373\n",
      "   LABEL = 2       0.89      0.94      0.91      8869\n",
      "\n",
      "    accuracy                           0.84     13512\n",
      "   macro avg       0.72      0.67      0.69     13512\n",
      "weighted avg       0.83      0.84      0.83     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 200, learning_rate = 0.5, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.47      0.33      0.39      1270\n",
      "   LABEL = 1       0.81      0.81      0.81      3373\n",
      "   LABEL = 2       0.90      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.69      0.70     13512\n",
      "weighted avg       0.84      0.85      0.84     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.05, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.59      0.03      0.06      1270\n",
      "   LABEL = 1       0.73      0.62      0.67      3373\n",
      "   LABEL = 2       0.80      0.95      0.87      8869\n",
      "\n",
      "    accuracy                           0.78     13512\n",
      "   macro avg       0.71      0.53      0.53     13512\n",
      "weighted avg       0.76      0.78      0.74     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.05, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.56      0.13      0.21      1270\n",
      "   LABEL = 1       0.77      0.74      0.75      3373\n",
      "   LABEL = 2       0.85      0.95      0.89      8869\n",
      "\n",
      "    accuracy                           0.82     13512\n",
      "   macro avg       0.72      0.61      0.62     13512\n",
      "weighted avg       0.80      0.82      0.79     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.05, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.52      0.24      0.33      1270\n",
      "   LABEL = 1       0.80      0.79      0.79      3373\n",
      "   LABEL = 2       0.88      0.95      0.91      8869\n",
      "\n",
      "    accuracy                           0.84     13512\n",
      "   macro avg       0.73      0.66      0.68     13512\n",
      "weighted avg       0.82      0.84      0.83     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.55      0.07      0.12      1270\n",
      "   LABEL = 1       0.73      0.68      0.70      3373\n",
      "   LABEL = 2       0.82      0.94      0.88      8869\n",
      "\n",
      "    accuracy                           0.79     13512\n",
      "   macro avg       0.70      0.56      0.57     13512\n",
      "weighted avg       0.77      0.79      0.76     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.53      0.20      0.29      1270\n",
      "   LABEL = 1       0.79      0.77      0.78      3373\n",
      "   LABEL = 2       0.87      0.95      0.91      8869\n",
      "\n",
      "    accuracy                           0.84     13512\n",
      "   macro avg       0.73      0.64      0.66     13512\n",
      "weighted avg       0.82      0.84      0.82     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.50      0.30      0.37      1270\n",
      "   LABEL = 1       0.81      0.81      0.81      3373\n",
      "   LABEL = 2       0.89      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.68      0.70     13512\n",
      "weighted avg       0.83      0.85      0.84     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.5, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.48      0.20      0.29      1270\n",
      "   LABEL = 1       0.77      0.77      0.77      3373\n",
      "   LABEL = 2       0.87      0.94      0.90      8869\n",
      "\n",
      "    accuracy                           0.83     13512\n",
      "   macro avg       0.70      0.64      0.65     13512\n",
      "weighted avg       0.81      0.83      0.81     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.5, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.49      0.34      0.40      1270\n",
      "   LABEL = 1       0.80      0.81      0.81      3373\n",
      "   LABEL = 2       0.90      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.70      0.71     13512\n",
      "weighted avg       0.84      0.85      0.84     13512\n",
      "\n",
      "GradientBoosting: n_estimators = 500, learning_rate = 0.5, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.46      0.36      0.41      1270\n",
      "   LABEL = 1       0.81      0.80      0.81      3373\n",
      "   LABEL = 2       0.90      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.70      0.71     13512\n",
      "weighted avg       0.84      0.85      0.84     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 0.1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.74      0.45      0.56      3373\n",
      "   LABEL = 2       0.75      0.96      0.84      8869\n",
      "\n",
      "    accuracy                           0.75     13512\n",
      "   macro avg       0.50      0.47      0.47     13512\n",
      "weighted avg       0.67      0.75      0.69     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 0.1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.54      0.01      0.03      1270\n",
      "   LABEL = 1       0.76      0.63      0.69      3373\n",
      "   LABEL = 2       0.80      0.96      0.87      8869\n",
      "\n",
      "    accuracy                           0.79     13512\n",
      "   macro avg       0.70      0.53      0.53     13512\n",
      "weighted avg       0.76      0.79      0.74     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 0.1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.60      0.07      0.13      1270\n",
      "   LABEL = 1       0.78      0.73      0.75      3373\n",
      "   LABEL = 2       0.83      0.96      0.89      8869\n",
      "\n",
      "    accuracy                           0.82     13512\n",
      "   macro avg       0.74      0.59      0.59     13512\n",
      "weighted avg       0.80      0.82      0.78     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.52      0.11      0.18      1270\n",
      "   LABEL = 1       0.73      0.72      0.73      3373\n",
      "   LABEL = 2       0.84      0.94      0.88      8869\n",
      "\n",
      "    accuracy                           0.81     13512\n",
      "   macro avg       0.70      0.59      0.60     13512\n",
      "weighted avg       0.78      0.81      0.78     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.51      0.27      0.35      1270\n",
      "   LABEL = 1       0.79      0.80      0.80      3373\n",
      "   LABEL = 2       0.89      0.94      0.91      8869\n",
      "\n",
      "    accuracy                           0.84     13512\n",
      "   macro avg       0.73      0.67      0.69     13512\n",
      "weighted avg       0.83      0.84      0.83     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.49      0.34      0.40      1270\n",
      "   LABEL = 1       0.80      0.81      0.81      3373\n",
      "   LABEL = 2       0.90      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.69      0.71     13512\n",
      "weighted avg       0.84      0.85      0.84     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 10, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.09      0.89      0.17      1270\n",
      "   LABEL = 1       0.33      0.13      0.18      3373\n",
      "   LABEL = 2       0.00      0.00      0.00      8869\n",
      "\n",
      "    accuracy                           0.12     13512\n",
      "   macro avg       0.14      0.34      0.12     13512\n",
      "weighted avg       0.09      0.12      0.06     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 10, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.07      0.00      0.01      1270\n",
      "   LABEL = 1       0.23      0.17      0.19      3373\n",
      "   LABEL = 2       0.65      0.81      0.72      8869\n",
      "\n",
      "    accuracy                           0.57     13512\n",
      "   macro avg       0.32      0.33      0.31     13512\n",
      "weighted avg       0.49      0.57      0.52     13512\n",
      "\n",
      "XGBoost: n_estimators = 100, learning_rate = 10, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.28      0.02      0.04      3373\n",
      "   LABEL = 2       0.66      0.98      0.79      8869\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.31      0.33      0.28     13512\n",
      "weighted avg       0.50      0.65      0.53     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 0.1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.53      0.01      0.02      1270\n",
      "   LABEL = 1       0.73      0.56      0.64      3373\n",
      "   LABEL = 2       0.77      0.95      0.85      8869\n",
      "\n",
      "    accuracy                           0.77     13512\n",
      "   macro avg       0.68      0.51      0.50     13512\n",
      "weighted avg       0.74      0.77      0.72     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 0.1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.58      0.06      0.11      1270\n",
      "   LABEL = 1       0.76      0.71      0.73      3373\n",
      "   LABEL = 2       0.83      0.95      0.88      8869\n",
      "\n",
      "    accuracy                           0.81     13512\n",
      "   macro avg       0.72      0.57      0.58     13512\n",
      "weighted avg       0.78      0.81      0.77     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 0.1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.56      0.16      0.24      1270\n",
      "   LABEL = 1       0.78      0.78      0.78      3373\n",
      "   LABEL = 2       0.86      0.95      0.91      8869\n",
      "\n",
      "    accuracy                           0.84     13512\n",
      "   macro avg       0.74      0.63      0.64     13512\n",
      "weighted avg       0.81      0.84      0.81     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.54      0.18      0.27      1270\n",
      "   LABEL = 1       0.76      0.75      0.76      3373\n",
      "   LABEL = 2       0.86      0.94      0.90      8869\n",
      "\n",
      "    accuracy                           0.82     13512\n",
      "   macro avg       0.72      0.62      0.64     13512\n",
      "weighted avg       0.80      0.82      0.80     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.50      0.32      0.39      1270\n",
      "   LABEL = 1       0.80      0.81      0.81      3373\n",
      "   LABEL = 2       0.90      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.69      0.70     13512\n",
      "weighted avg       0.84      0.85      0.84     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.49      0.38      0.43      1270\n",
      "   LABEL = 1       0.81      0.82      0.82      3373\n",
      "   LABEL = 2       0.91      0.94      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.74      0.71      0.72     13512\n",
      "weighted avg       0.85      0.85      0.85     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 10, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.09      0.89      0.17      1270\n",
      "   LABEL = 1       0.33      0.13      0.18      3373\n",
      "   LABEL = 2       0.00      0.00      0.00      8869\n",
      "\n",
      "    accuracy                           0.12     13512\n",
      "   macro avg       0.14      0.34      0.12     13512\n",
      "weighted avg       0.09      0.12      0.06     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 10, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.07      0.00      0.01      1270\n",
      "   LABEL = 1       0.23      0.17      0.19      3373\n",
      "   LABEL = 2       0.65      0.81      0.72      8869\n",
      "\n",
      "    accuracy                           0.57     13512\n",
      "   macro avg       0.32      0.33      0.31     13512\n",
      "weighted avg       0.49      0.57      0.52     13512\n",
      "\n",
      "XGBoost: n_estimators = 200, learning_rate = 10, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.28      0.02      0.04      3373\n",
      "   LABEL = 2       0.66      0.98      0.79      8869\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.31      0.33      0.28     13512\n",
      "weighted avg       0.50      0.65      0.53     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 0.1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.60      0.05      0.08      1270\n",
      "   LABEL = 1       0.73      0.66      0.69      3373\n",
      "   LABEL = 2       0.81      0.95      0.87      8869\n",
      "\n",
      "    accuracy                           0.79     13512\n",
      "   macro avg       0.71      0.55      0.55     13512\n",
      "weighted avg       0.77      0.79      0.75     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 0.1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.56      0.15      0.24      1270\n",
      "   LABEL = 1       0.77      0.77      0.77      3373\n",
      "   LABEL = 2       0.86      0.95      0.90      8869\n",
      "\n",
      "    accuracy                           0.83     13512\n",
      "   macro avg       0.73      0.62      0.64     13512\n",
      "weighted avg       0.81      0.83      0.81     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 0.1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.54      0.25      0.34      1270\n",
      "   LABEL = 1       0.80      0.81      0.80      3373\n",
      "   LABEL = 2       0.89      0.95      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.74      0.67      0.69     13512\n",
      "weighted avg       0.83      0.85      0.83     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 1, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.52      0.24      0.33      1270\n",
      "   LABEL = 1       0.78      0.79      0.78      3373\n",
      "   LABEL = 2       0.88      0.94      0.91      8869\n",
      "\n",
      "    accuracy                           0.84     13512\n",
      "   macro avg       0.73      0.66      0.67     13512\n",
      "weighted avg       0.82      0.84      0.82     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 1, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.49      0.37      0.42      1270\n",
      "   LABEL = 1       0.81      0.81      0.81      3373\n",
      "   LABEL = 2       0.90      0.93      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.71      0.72     13512\n",
      "weighted avg       0.84      0.85      0.84     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 1, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.48      0.40      0.43      1270\n",
      "   LABEL = 1       0.81      0.81      0.81      3373\n",
      "   LABEL = 2       0.91      0.93      0.92      8869\n",
      "\n",
      "    accuracy                           0.85     13512\n",
      "   macro avg       0.73      0.72      0.72     13512\n",
      "weighted avg       0.85      0.85      0.85     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 10, max_depth = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.09      0.89      0.17      1270\n",
      "   LABEL = 1       0.33      0.13      0.18      3373\n",
      "   LABEL = 2       0.00      0.00      0.00      8869\n",
      "\n",
      "    accuracy                           0.12     13512\n",
      "   macro avg       0.14      0.34      0.12     13512\n",
      "weighted avg       0.09      0.12      0.06     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 10, max_depth = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.07      0.00      0.01      1270\n",
      "   LABEL = 1       0.23      0.17      0.19      3373\n",
      "   LABEL = 2       0.65      0.81      0.72      8869\n",
      "\n",
      "    accuracy                           0.57     13512\n",
      "   macro avg       0.32      0.33      0.31     13512\n",
      "weighted avg       0.49      0.57      0.52     13512\n",
      "\n",
      "XGBoost: n_estimators = 500, learning_rate = 10, max_depth = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LABEL = 0       0.00      0.00      0.00      1270\n",
      "   LABEL = 1       0.28      0.02      0.04      3373\n",
      "   LABEL = 2       0.66      0.98      0.79      8869\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.31      0.33      0.28     13512\n",
      "weighted avg       0.50      0.65      0.53     13512\n",
      "\n",
      "the accuracy of the best model which is GradientBoosting: n_estimators = 100, learning_rate = 0.5, max_depth = 7 on the test set is: 0.8415482534043813, with accuracy as the metric\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "target_name = ['LABEL = 0', 'LABEL = 1', 'LABEL = 2']\n",
    "accuracy_and_hyperparameters = []\n",
    "\n",
    "def acc_from_classification_report(cls_rep):\n",
    "    \"\"\"\n",
    "    :param cls_rep: classification report\n",
    "    :return: accuracy from the classification report\n",
    "    \"\"\"\n",
    "    acc = float(cls_rep.split('accuracy')[1].split()[0])\n",
    "    return acc\n",
    "    \n",
    "\n",
    "# LogisticRegression\n",
    "hyper_param = {'C': [0.1, 1, 10], 'max_iter': [100, 1000, 2500]}\n",
    "combinations = list(itertools.product(*hyper_param.values()))\n",
    "for comb in combinations:\n",
    "    clf = LogisticRegression(C=comb[0], max_iter=comb[1]).fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "    name = f\"LogisticRegression: penalty = C = {comb[0]}, max_iter = {comb[1]}\"\n",
    "    print(name)\n",
    "    cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "    print(cls_rep)\n",
    "    # save the accuracy and hyperparameters from the classification report\n",
    "    acc = acc_from_classification_report(cls_rep)\n",
    "    accuracy_and_hyperparameters.append([name, acc, clf])\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "clf = GaussianNB().fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "print(\"Naive Bayes\")\n",
    "cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "print(cls_rep)\n",
    "acc = acc_from_classification_report(cls_rep)\n",
    "accuracy_and_hyperparameters.append([\"Naive Bayes\", acc, clf])\n",
    "\n",
    "\n",
    "# KNN\n",
    "hyper_param = {'n_neighbors': [3, 5, 7]}\n",
    "combinations = list(itertools.product(*hyper_param.values()))\n",
    "for comb in combinations:\n",
    "    clf = KNeighborsClassifier(n_neighbors=comb[0]).fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "    name = f\"KNN: n_neighbors = {comb[0]}\"\n",
    "    print(name)\n",
    "    cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "    print(cls_rep)\n",
    "    acc = acc_from_classification_report(cls_rep)\n",
    "    accuracy_and_hyperparameters.append([name, acc, clf])\n",
    "\n",
    "\n",
    "# # SVM\n",
    "# hyper_param = {'C': [0.1, 1, 10], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "# clf = SVC().fit(X_train_scaled, y_train)\n",
    "# y_pred = clf.predict(X_dev_scaled)\n",
    "\n",
    "# RF\n",
    "hyper_param = {'n_estimators': [100, 200, 500], 'max_depth': [3, 5, 7]}\n",
    "combinations = list(itertools.product(*hyper_param.values()))\n",
    "for comb in combinations:\n",
    "    clf = RandomForestClassifier(n_estimators=comb[0], max_depth=comb[1]).fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "    name = f\"RandomForest: n_estimators = {comb[0]}, max_depth = {comb[1]}\"\n",
    "    print(name)\n",
    "    cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "    print(cls_rep)\n",
    "    acc = acc_from_classification_report(cls_rep)\n",
    "    accuracy_and_hyperparameters.append([name, acc, clf])\n",
    "\n",
    "# Decision Tree\n",
    "hyper_param = {'max_depth': [3, 5, 7]}\n",
    "combinations = list(itertools.product(*hyper_param.values()))\n",
    "for comb in combinations:\n",
    "    clf = DecisionTreeClassifier(max_depth=comb[0]).fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "    name = f\"DecisionTree: max_depth = {comb[0]}\"\n",
    "    print(name)\n",
    "    cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "    print(cls_rep)\n",
    "    acc = acc_from_classification_report(cls_rep)\n",
    "    accuracy_and_hyperparameters.append([name, acc, clf])\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "hyper_param = {'n_estimators': [100, 200, 500], 'learning_rate': [0.1, 1, 10]}\n",
    "combinations = list(itertools.product(*hyper_param.values()))\n",
    "for comb in combinations:\n",
    "    clf = AdaBoostClassifier(n_estimators=comb[0], learning_rate=comb[1]).fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "    name = f\"AdaBoost: n_estimators = {comb[0]}, learning_rate = {comb[1]}\"\n",
    "    print(name)\n",
    "    cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "    print(cls_rep)\n",
    "    acc = acc_from_classification_report(cls_rep)\n",
    "    accuracy_and_hyperparameters.append([name, acc, clf])\n",
    "\n",
    "\n",
    "# GradientBoosting\n",
    "hyper_param = {'n_estimators': [100, 200, 500], 'learning_rate': [0.05, 0.1, 0.5], 'max_depth': [3, 5, 7]}\n",
    "combinations = list(itertools.product(*hyper_param.values()))\n",
    "for comb in combinations:\n",
    "    clf = GradientBoostingClassifier(n_estimators=comb[0], learning_rate=comb[1], max_depth=comb[2]).fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "    name = f\"GradientBoosting: n_estimators = {comb[0]}, learning_rate = {comb[1]}, max_depth = {comb[2]}\"\n",
    "    print(name)\n",
    "    cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "    print(cls_rep)\n",
    "    acc = acc_from_classification_report(cls_rep)\n",
    "    accuracy_and_hyperparameters.append([name, acc, clf])\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "hyper_param = {'n_estimators': [100, 200, 500], 'learning_rate': [0.1, 1, 10], 'max_depth': [3, 5, 7]}\n",
    "combinations = list(itertools.product(*hyper_param.values()))\n",
    "for comb in combinations:\n",
    "    clf = XGBClassifier(n_estimators=comb[0], learning_rate=comb[1], max_depth=comb[2]).fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "    name = f\"XGBoost: n_estimators = {comb[0]}, learning_rate = {comb[1]}, max_depth = {comb[2]}\"\n",
    "    print(name)\n",
    "    cls_rep = classification_report(y_dev, y_pred, target_names=target_name)\n",
    "    print(cls_rep)\n",
    "    acc = acc_from_classification_report(cls_rep)\n",
    "    accuracy_and_hyperparameters.append([name, acc, clf])\n",
    "    \n",
    "\n",
    "# print the best accuracy and hyperparameters from all the models\n",
    "best_acc = max(accuracy_and_hyperparameters, key=lambda x: x[1])\n",
    "best_name = best_acc[0]\n",
    "\n",
    "\n",
    "print(f\"the accuracy of the best model which is {best_name} on the test set is: {best_acc[-1].score(X_test_scaled, y_test)}, with accuracy as the metric\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation of the chosen model:\n",
    "We chose the best model due to the accuracy metric from the classification report.\n",
    "We chose this metric because we wanted to predict with generic metric.\n",
    "\n",
    "In all the models we chose the hyperparameters that include the default values.\n",
    "\n",
    "In XGBoost we chose those hyperparameters because we wanted not to overfit the model, and we wanted to get the best accuracy, so we limited the number of estimators and the max depth of the tree.\n",
    "\n",
    "As we can see from the classification reports, the worst model on the dev set was the Naive Bayes model, and the best model was the XGBoost model.\n",
    "\n",
    "In addition, we can see the following:\n",
    "XGBoost: n_estimators = 500, learning_rate = 10, max_depth = 3: 0.12 accuracy on the dev set\n",
    "XGBoost: n_estimators = 200, learning_rate = 10, max_depth = 3: 0.12 accuracy on the dev set\n",
    "XGBoost: n_estimators = 100, learning_rate = 10, max_depth = 3: 0.12 accuracy on the dev set\n",
    "So we can see that the number of estimators doesn't affect the accuracy of the model when the learning rate is 10 and the max depth is 3.\n",
    "This is happening because the learning rate is too high, so the model is not learning from the data, and the max depth is too low, so the model is not learning from the data.\n",
    "\n",
    "We can see the accuracy of the best model on the dev set is 0.85 and there are some that are close to it (or even equal), so maybe we could get better result with the others on the test set.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
